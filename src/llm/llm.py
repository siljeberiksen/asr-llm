## This is called directly into the Whisper model ##

import json
import os
import re
import requests
from dotenv import load_dotenv
load_dotenv() 

HOSTNAME = os.environ["HOSTNAME"]
PORT = os.environ["PORT"]

url = f"http://{HOSTNAME}:{PORT}/completion"  # llama.cpp server
headers = {"Content-Type": "application/json"}

only_string_schema = {
    "type": "string"
}
only_number_schema = {
    "type": "number",
    "enum": [1, 2, 3, 4, 5]
}

both_schema = {
    "type": "object",
    "properties": {
        "string_field": {
            "type": "string"
        },
        "number_field": {
            "type": "number",
            "enum": [1, 2, 3, 4, 5]
        }
    },
    "required": ["string_field", "number_field"]
}

schemas = {
    "default": only_string_schema,
    "number_only": only_number_schema,
    "both": both_schema
}

def pred(
    instruction,
    max_tokens=1000,
    use_schema: str = "default",
    temp=0.1,  # temperature. 0: deterministic, 1+: random
    # min_p=0.1,  # minimum probability
    # max_p=0.9,  # maximum probability
    # top_p=0.9,  # nucleus sampling
    # top_k=40,  # consider top k tokens at each generation step
    evaluate: bool = False,  # apply eval
):
    if len(instruction) == 0:
        raise ValueError("Instruction cannot be empty")

    data = {
        "prompt": instruction,
        "n_predict": max_tokens,
        "temperature": temp,
        "repeat_penalty": 1.2,  # 1.1 default,
    }
    if use_schema:
        data["json_schema"] = schemas[use_schema]

    response = requests.post(url, headers=headers, data=json.dumps(data)).json()
    response = response["content"]
    if evaluate:
        try:
            return parse_llm_output(response)
        except:
            print("ERROR")
            print(response)
            pred(instruction, evaluate=True)
    return response

def parse_llm_output(response: str):
    if not response:
        return response
    # spacing!
    response = response.replace("\n", " ")
    response = response.replace("\t", " ")
    response = re.sub(r"\s+", " ", response)
    response = response.strip()
    # markdown ticks
    response = response.replace("```python", "")
    response = response.replace("```json", "")
    response = response.replace("```", "")

    response = response.replace("false", "False")
    response = response.replace("true", "True")
    response = response.replace("null", "None")
    response = response.replace("The selected top1 ASR transcription", "")

    #response = response.lower()

    # Regular expression to capture text between <optionnumber> tags
    match = re.search(r'<option\d+>(.*?)</option\d+>',response)

    # Remove any HTML code still left
    # TODO: This did not acutally work propably!!
    CLEANR = re.compile('<.*?>')
    if match:
        extracted_text = match.group(1)
        cleantext = re.sub(CLEANR, '', extracted_text)
        return cleantext.strip()

    else:
        raise Exception("No option returned")


def read_file(file_path):
    try:
        with open(file_path, 'r') as file:
            data = json.load(file)
    except FileNotFoundError:
        data = []  # If file doesn't exist, initialize with an empty list
    return data
    

def choose_best_sentence(context, choices):
    prompt = "You are an ASR transcript selector." 
    prompt += "Perform language model rescoring based on the top5 outputs generated by an Automatic Speech Recognition (ASR) system given a converstional history\n"
    prompt +=f"History:  [{', '.join(context)}]\n\n"
    prompt += "The ASR hypotheses are as follows:\n"
    for i, choice in enumerate(choices, 1):
        prompt += f"<option{i}> {choice} </option{i}>\n"

    prompt += (
     "Output the selected top1 ASR transcription in the format: <option?> The selected top1 ASR transcription </option?>\n"
     "Do NOT iclude your reasoning"
    )
    print("Prompt:", prompt)
    return pred(prompt, evaluate=True)